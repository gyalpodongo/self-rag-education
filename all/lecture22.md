#lecture22

##SLIDES

###slide 0
BIG OH and THETA
(download slides and . pyfiles to follow along)
6.100L Lecture 22
Ana Bell

###slide 1
TIMING AND COUNTING ARE OK
…BUT NEED A BETTER WAY
•Timing and counting evaluate implementations
•Timing and counting evaluate machines
•Want to evaluate algorithm
•Want to evaluate scalability
•Want to evaluate in terms of input size
6.100L Lecture 21


###slide 2
ORDER of GROWTH
6.100L Lecture 22

###slide 3
ORDERS OF GROWTH
It’s a notation
Evaluates programs when input is very big
Expresses the growth of program ’s run time
Puts an upper bound on growth
Do not need to be precise: “order of ” not “exact ” growth
Focus on the largest factors in run time (which section of 
the program will take the longest to run?)
6.100L Lecture 22

###slide 4
A BETTER WAY
A GENERALIZED WAY WITH APPROXIMATIONS
Use the idea of counting operations in an algorithm, but not 
worry about small variations in implementation
When x is big, 3x+4 and 3x and x are pretty much the same! 
Don’t care about exact value: ops = 1+x(2+1) 
Express it in an “order of” way vs. the input : ops = Order of x
Focus on how algorithm performs when size of problem gets arbitrarily large
Relate time needed to complete a computation against the 
size of the input to the problem
Need to decide what to measure. What is the input?
6.100L Lecture 22

###slide 5
6.0001 LECTURE 8WHICH INPUT TO USE TO MEASURE EFFICIENCY
Want to express efficiency in terms of input, so need to 
decide what is your input
Could be an integer 
--convert_to_km(x)
Could be length of list 
--list_sum(L)
You decide when multiple parameters to a function
--is_in(L, e)
Might be different depending on which input you consider
6.100L Lecture 22

###slide 6
6.0001 LECTURE 8DIFFERENT INPUTS CHANGE HOW 
THE PROGRAM RUNS
A function that searches for an element in a list
defis_in(L, e):
foriinL:
ifi== e:
returnTrue
returnFalse
Does the program take longer to run as eincreases ?
No 
6.100L Lecture 22

###slide 7
6.0001 LECTURE 8DIFFERENT INPUTS CHANGE HOW 
THE PROGRAM RUNS
A function that searches for an element in a list
defis_in(L, e):
foriinL:
ifi== e:
returnTrue
returnFalse
Does the program take longer to run as L increases?
What if L has a fixed length and its elements are big numbers ?
No 
What if L has different lengths ? 
Yes!
6.100L Lecture 22

###slide 8
6.0001 LECTURE 8DIFFERENT INPUTS CHANGE HOW 
THE PROGRAM RUNS
A function that searches for an element in a list
defis_in(L, e):
foriinL:
ifi== e:
returnTrue
returnFalse
When eis first element in the list 
 BEST CASE
When look through about half of the elements in list
 AVERAGE CASE
When eis not in list 
 WORST CASE
Want to measure this behavior in a general way
6.100L Lecture 22

###slide 9
6.0001 LECTURE 8ASYMPTOTIC GRO WTH
Goal: describe how time grows as size of input grows
Formula relating input to number of operations
Given an expression for the number of operations needed to 
compute an algorithm, want to know asymptotic behavior as size 
of problem gets large
Want to put a bound on growth
Do not need to be precise: “order of ” not “exact ” growth
Will focus on term that grows most rapidly
Ignore additive and multiplicative constants, since want to know how 
rapidly time required increases as we increase size of input
This is called order of growth
Use mathematical notions of “ big O ” and “big Θ ”
Big Oh  and Big Theta
6.100L Lecture 22


###slide 10
6.100L Lecture 22


###slide 11
BIG O Definition 
𝑔𝑔(𝑥𝑥)=𝑥𝑥Not an upper bound; as 𝑥𝑥→∞
f(x) will always exceed it
4𝑥𝑥2>3𝑥𝑥2+20𝑥𝑥+1∀𝑥𝑥>20.04
Suppose  some code runs in 
𝑓𝑓(𝑥𝑥)=3𝑥𝑥2+20𝑥𝑥+1steps
Think of this as the formula from 
counting the number of ops.
Big O His a way to upper bound the 
growth of any function
f(x) = O(g(x)) means that g(x) times 
some constant eventually always 
exceeds f(x)
Eventually means above some 
threshold value of xCrossoverNever 
cross again!3𝑥𝑥2+20𝑥𝑥+1=𝑂𝑂(𝑥𝑥2)
6.100L Lecture 22

###slide 12
BIG O FORMALLY
A big Oh bound is an upper bound on the growth of some function
𝑓𝑓(𝑥𝑥)=𝑂𝑂(𝒈𝒈(𝒙𝒙))means there exist 
constants 𝒄𝒄𝟎𝟎,𝒙𝒙𝟎𝟎for which 𝒄𝒄𝟎𝟎𝒈𝒈(𝒙𝒙)≥𝒇𝒇(𝒙𝒙)for all 𝑥𝑥 >𝒙𝒙𝟎𝟎
Example:  𝑓𝑓(𝑥𝑥)=3𝑥𝑥2+20𝑥𝑥+1
𝑓𝑓(𝑥𝑥)=𝑂𝑂(𝒙𝒙𝟐𝟐) ,because 𝟒𝟒𝒙𝒙𝟐𝟐>𝟑𝟑𝒙𝒙𝟐𝟐+𝟐𝟐𝟎𝟎𝒙𝒙+𝟏𝟏∀𝑥𝑥≥𝟐𝟐𝟏𝟏
(𝒄𝒄𝟎𝟎=𝟒𝟒,𝒙𝒙𝟎𝟎=𝟐𝟐𝟎𝟎.𝟎𝟎𝟒𝟒)
0 <= x <= 30 0 <= x <= 100Crossover at 
x=20.04
These lines
will nevercross againorange > blue 
for all x > 20.04)
6.100L Lecture 22

###slide 13
0 <= x <= 100
BIG Θ Definition
A big Θ bound is a lower and upper bound on the growth of some function
Suppose  𝑓𝑓(𝑥𝑥)=3𝑥𝑥2−20𝑥𝑥−1
𝒇𝒇(𝒙𝒙)=Θ(𝒈𝒈(𝒙𝒙))means:
there exist constants 𝒄𝒄𝟎𝟎,𝑥𝑥0for which 𝒄𝒄 𝟎𝟎𝒈𝒈(𝒙𝒙)≥𝒇𝒇(𝒙𝒙)for all 𝑥𝑥>𝒙𝒙𝟎𝟎
and             constants 𝒄𝒄𝟏𝟏,𝑥𝑥1for which 𝒄𝒄 𝟏𝟏𝒈𝒈(𝒙𝒙)≤𝒇𝒇(𝒙𝒙)for all 𝑥𝑥>𝒙𝒙𝟏𝟏
Example, 𝒇𝒇(𝒙𝒙)=Θ(𝒙𝒙𝟐𝟐)because    𝟒𝟒𝒙𝒙𝟐𝟐>𝟑𝟑𝒙𝒙𝟐𝟐−𝟐𝟐𝟎𝟎𝒙𝒙−𝟏𝟏∀𝑥𝑥≥𝟎𝟎(𝒄𝒄𝟎𝟎=𝟒𝟒,𝒙𝒙𝟎𝟎=𝟎𝟎)
and  𝟐𝟐𝒙𝒙𝟐𝟐<𝟑𝟑𝒙𝒙𝟐𝟐−𝟐𝟐𝟎𝟎𝒙𝒙−𝟏𝟏∀𝑥𝑥≥𝟐𝟐𝟏𝟏(𝒄𝒄𝟏𝟏=𝟐𝟐,𝒙𝒙𝟏𝟏=𝟐𝟐𝟎𝟎.𝟎𝟎𝟒𝟒)
These lines
will nevercross againorange > blue 
for all x > 0
blue > green 
for all x > 20.043𝑥𝑥2−20𝑥𝑥−1=𝜃𝜃(𝑥𝑥2)
6.100L Lecture 22

###slide 14
6.100L Lecture 22


###slide 15
Θ vs O
In practice, Θ bounds are preferred, because they are “ tight ”
For example: 𝑓𝑓(𝑥𝑥)=3𝑥𝑥2−20𝑥𝑥−1
𝑓𝑓𝑥𝑥=𝑂𝑂𝑥𝑥2=𝑂𝑂𝑥𝑥3=𝑂𝑂(2𝑥𝑥)and anything higher order
because they all upper bound it
𝒇𝒇𝒙𝒙=𝜣𝜣𝒙𝒙𝟐𝟐
≠Θ𝑥𝑥3≠Θ2𝑥𝑥and anything higher order because they 
upper bound but not lower bound it 
6.100L Lecture 22

###slide 16
6.0001 LECTURE 8SIMPLIFICATION EXAMPLES
Drop constants and multiplicative factors
Focus on dominant term
: n2+ 2n + 2
: 3x2+ 100000x + 31000 
: log(a) + a+ 4Θ(n2)
Θ(x2)
Θ(a)
6.100L Lecture 22

###slide 17
BIG  IDEA
Express Theta in terms of 
the input.
Don’t just use n all the time!
6.100L Lecture 22

###slide 18
YOU TRY IT!
: 1000*log(x) + x
: n2log(n) + n3
: log(y) + 0.000001y
: 2b+ 1000a2 + 100*b2 + 0.0001a3
6.100L Lecture 22Θ(x)
Θ(n3)
Θ(y)
Θ(2b)
Θ(a3)
Θ(2b+a3)
All could be ok, depends on the input we care about

###slide 19
6.0001 LECTURE 8USING Θ TO EVALUATE YOUR 
ALGORITHM
deffact_iter (n):
"""assumes n an int >= 0"""
answer = 1whilen > 1:
answer *= nn -= 1
returnanswer
Number of steps: 
Worst case asymptotic complexity: 
Ignore additive constants
2 doesn’t matter when n is big
Ignore multiplicative constant s
5 doesn’t matter if just want to know how increasing n changes time 
needed5n + 2
Θ(n)
6.100L Lecture 22

###slide 20
6.0001 LECTURE 8COMBINING C OMPLEXITY CLASSES
LOOPS IN SERIES
Analyze statements inside functions to get order of growth
Apply some rules, focus on dominant term
Law of Addition for Θ(): 
Used with sequential statements
Θ(𝑓𝑓(𝑛𝑛))+Θ(𝑔𝑔(𝑛𝑛))=Θ(𝑓𝑓(𝑛𝑛)+𝑔𝑔(𝑛𝑛))
For example, 
foriinrange(n):
print('a')
forj inrange(n*n):
print('b')
is Θ(𝑛𝑛)+Θ(𝑛𝑛∗𝑛𝑛)=Θ(𝑛𝑛+𝑛𝑛2)=Θ(𝑛𝑛2)because of   
dominant 𝑛𝑛2termΘ(n)
Θ(n2)
6.100L Lecture 22

###slide 21
6.0001 LECTURE 8COMBINING COMPLEXITY CLASSES
NESTED LOOPS
Analyze statements inside functions to get order of growth
Apply some rules, focus on dominant term
Law of Multiplication for Θ(): 
Used with nested statements/loops
Θ𝑓𝑓𝑛𝑛∗Θ(𝑔𝑔(𝑛𝑛))=Θ(𝑓𝑓𝑛𝑛∗𝑔𝑔(𝑛𝑛))
For example, 
foriinrange(n):
forj inrange(n//2 ):
print('a')
Θ(𝑛𝑛)×Θ(𝑛𝑛)=Θ(𝑛𝑛×𝑛𝑛)=Θ(𝑛𝑛2)
Outer loop runs n times and the inner loop runs n times 
for every outer loop iteration.Θ(n)
Θ(n) for each outer loop iteration
6.100L Lecture 22

###slide 22
ANALYZE COMPLEXITY
What is the Theta complexity of this program?
deff(x):
answer = 1foriin range(x):
forj in range( i,x):
answer += 2
returnanswer
Θ(1)+ Θ(x)*Θ(x)*Θ(1)+Θ(1)
Overall complexity is Θ(x2)by rules of addition and 
multiplication
6.100L Lecture 22Outer loop is Θ(x)
Inner loop is Θ( x)
Everything else is  Θ( 1)

###slide 23
YOU TRY IT!
What is the Theta complexity of this program? Careful to 
describe in terms of input (hint: what matters with a list, size of elems of length?)
deff(L):Lnew= []
foriinL:
Lnew.append (i**2)
returnLnew
6.100L Lecture 22ANSWER:
Loop: Θ(len(L))
f is Θ (len(L))

###slide 24
YOU TRY IT!
What is the Theta complexity of this program?
deff(L, L1, L2):
""" L, L1, L2 are the same length """
inL1 = False
foriinrange(len(L)):
ifL[i] == L1[i]:
inL1 = True
inL2 = False
foriinrange(len(L)):
ifL[i] == L2[i]:
inL2 = True
returninL1 andinL2
6.100L Lecture 22ANSWER:
Loop: Θ(len(L)) + Θ(len(L))
f is Θ (len(L)) or Θ(len(L1)) or Θ(len(L2))

###slide 25
6.0001 LECTURE 8
COMPLEXITY CLASSES
We want to design algorithms that are as 
close to top of this hierarchy as possible
6.100L Lecture 22Θ(1)denotes constant running time
Θ(log n) denotes logarithmic running time
Θ(n) denotes linear running time
Θ(n log n) denotes log-linear running time
Θ(nc) denotes polynomial running time 
(c is a constant)
Θ(cn) denotes exponential running time 
(c is a constant raised to a power based on input size)

###slide 26
COMPLEXITY GROWTH
CLASS N =10 N = 100 N = 1000 N = 1000000
Constant 1 1 1 1
Logarithmic 1 2 3 6
Linear 10 100 1000 1000000
Log-linear 10 200 3000 6000000
Polynomial 100 10000 1000000 1000000000000
Exponential 1024 12676506
002282294014967032053761071508607186267320948425
049060001810561404811705533607443750388370351051124936122493198378815695858
1275946729175531468251871
452856923140435984577574698574803934567774824230985421074605062371141877954
1821530464749835819412673
98767559165543946077062914571196477686542167660429831652624386837205668069376Good Luck!!
6.100L Lecture 22

###slide 27
SUMMARY
Timing is machine/implementation/algorithm dependent
Counting ops is implementation/algorithm dependent
Order of growth is algorithm dependent
Compare efficiency of algorithms
•Notation that describes growth
•Lower order of growth is better
•Independent of machine or specific implementation
Using Theta
•Describe asymptotic order of growth
•Asymptotic notation
•Upper bound and a lower bound
6.100L Lecture 22

##TRANSCRIPT

THETA TIMING TIMING A PROGRAM nvert EXAMPLE: convert to km, compound CREATING AN INPUT LIST RUN IT! convert_to_km OBSERVATIONS MEASURE TIME: compound with a variable number of months MEASURE TIME: sum over L s = sum of (L) print (f timesperfcount MEASURE TIME: find element in a list MEASURE TIME: diameter function TWO DIFFERENT MACHINES DON'T GET ME WRONG! COUNT OPERATIONS COUNT OPERA NS: is_in NT OPERATIONS: is in PLOT OF INPUT SIZE vs. OPERATION COUNT PROBLEMS WITH TIMING AND COUNTING EFFICIENCY IN TERMS OF INPUT: BIG-PICTURE ORDERS OF GROWTH A BETTER WAY A GENERALIZED WAY WITH APPROXIMATIONS WHICH INPUT TO USE TO MEASURE EFFICIENCY DIFFERENT INPUTS CHANGE HOW THE PROGRAM RUNS DIFFERENT INPUTS THE PROGRAM CHANGE HOW ASYMPTOTIC GROWTH BIG 0 Definition big Oh bound is an (x) = 0 (g (x)) mea constants co, x0 for… A big 0 BIG 0 FORMALLY Example big 0 bound is a lower a ose f (x) = 3x2 — 20x — 1 SIMPLIFICATION EXAMPLES YOU TRY IT! (co_C (e; USING 0 TO EVALUATE YOUR ALGORITHM COMBINING COMPLEXITY CLASSES LOOPS IN SERIES COMBINING COMPLEXITY CLASSES NESTED LOOPS ANALYZE COMPLEXITY COMPLEXITY CLASSES COMPLEXITY GROWTH SUMMARY All right, let's get started. Last lecture, we began talking about an entirely new topic in computer science, and we have begun learning about how to figure out the runtime of our programs. Right. So we did. We looked at how to actually time the program by figuring out exactly how long it takes and then how to count the number of operations in the program. Today, we're going to do a very same thing to begin with. So for the first half of the lecture, we'll time a bunch of programs and then we'll count the number of operations just like before. But we're going to do them in the context of slightly different, slightly more interesting programs or functions involving just pure numbers as our parameters and then functions that involve lists as our parameters. That will be the first half of the lecture. And then from there on, we're going to look at the idea of order of growth, which is kind of what we're building up, the set of lectures to, and then the order of growth. There'll be a little bit of math, a little bit of graphing, but not not too much. And then we're just we're just going to see how to actually evaluate the order of growth of functions from there on out. Okay. So let's begin by just figuring out the runtime of our programs, right? This was a really quick and easy way to for us to figure out exactly how long our programs take. So last lecture, we imported this time module and we're doing that again this time. But instead of actually running the, the time function that we had seen last lecture here, right, instead of running the time function which gave us this sort of global absolute time since some, some date in the past, we're going to run this slightly different function called performance counter. And this is what is typically used in the real world to figure out how long an actual program or a function takes to run. The reason we're using this is because it's more accurate. So the time time function that we used last lecture gave us maybe precision to one times 10 to -3 or something very, very big like that. The performance counter can actually give us precision to something that's a lot, lot smaller, so maybe one times ten, negative eight or something very small. So we'll be able to see the timings of some functions that were basically zero in the last lecture. Okay. So just a quick review of how we actually get the time that a function takes to run. We run this performance counter time and this one gives us not an absolute time but more of a a shorter time frame, not from some time in the past. And the performance counter is very useful when we're getting these details right. The difference in some times. So we're running the performance counter to get the quote unquote starting time. We run the function, we run the performance counter again to get the quote unquote, stopping time, subtract the starting time to get the data. Okay. So and then we will print that, that time to see how long the function actually takes to run. Oh, yeah, that's what I said. Okay. So we're going to look at two different functions than than last time. But they're going to they're going to have sort of the same overarching themes that we saw last lecture. So the first function we're going to look at is called convert to kilometers, taking in some miles and returns the value in kilometers. And the second function is a function named compound. So this one should seem very familiar. It will bring flashbacks to problems at one. It's a function that takes in a monthly investment, an interest rate for the month and some number of months to invest that much. And it returns how much money you've made over those number of months. So you can see here you have in total initialized a loop that goes through that many months and it updates the total based on the interest rate and how much money you have there right now, plus whatever you've invested for that month. Okay. So the three questions we're going to answer, just like we answered last lecture, is how long natural seconds does it take to run these functions? Which input parameters does the function actually depend on? And do these two functions actually run for different amounts of time? And sort of what is that difference? Right? Does one run in 12 seconds and the other one one run in point five? So what what is the actual time that it takes for them to run? So this is our code. So these are the two functions. Before we go on, let me just show you how we're creating the inputs. So just like before, we're creating a list of all of the different inputs we're going to run the function with. So here I've got this l n that will contain the numbers 110, 100, 1000 and so on. And these are going to be the parameters to my function one at a time, of course. And then I've got my loop here for each one of those inputs. 110, 100 a thousand. I'm just going to run my function, right? So here I'm measuring the time it takes and then I'm going to report that the time that it took to run the program. And just for fun. I'm also going to report how many times this program could run in one second. Because for me, it was a little bit hard to read, you know, one time sudden negative, you know, 2058 or something like that. But it was a lot easier for me to see this big number for how many times that function could have run in one second. So here I've got. Convert to kilometers. So I'm going to run it and we're going to see it's this right here. How long the function actually took. So last time we ran a program that was really simple like this, all of it basically said it took 0 seconds. Right? It was just so fast that that time that time function was wasn't able to pick up that precise time difference. But this performance counter can. Right. Which is a lot nicer. So now we see that no matter what the input, it looks like the time is pretty much the same, right? Three times negative 7 seconds, no matter what the input is. Right. That was expected. Now, what about the. The compound function? This one's going to be a little bit more interesting because there are actually three parameters to this function. Right. So what we're going to do is change each one and see which one of those parameters actually has an effect on the runtime. So here, this bit is going to fix my interest rate and fixed the number of months and I'm going to change the amount I invest every month. Okay. So if I run that, that was pretty fast. Again, we look at the results here and no matter how much I invest every month, it looks like the program doesn't really change how long it takes to run. Right. It's always about one times ten to the negative 6 seconds to run. All right. What if I change the interest rate? So this one was a little bit harder to change, but I settled on this as the thing I'm varying. Sorry. I'm varying it in this way. So it's going to be 1.1.1 or 1.01 or 1.01. That's what I'm going to invest that the interest rate for whatever I'm going to invest in and I'm going to fix $10 is my investment per month and fix the 12 months again. So if I run that. Same deal. It looks like changing this investment isn't really making much of a difference in how long it takes the program to run. All right. One last parameter to try. So now I'm going to fix the initial investment to $10 a month, and I'm going to fix my interest rate to this per month. And I'm going to vary how many months I'm going to invest this. So again, this end will be one O ten, 100, 1000, 10,000, and so on. So let's see what this is going to do. Already. It's doing something different than the other two because it hasn't finished running yet. Right. So it's still working on this last one down here. But we can see that more interesting things are happening now. Right. So here I've got initially it's a little bit hard to tell for those small numbers, which is fine. But luckily we're able to run it for a bunch of a bunch of inputs. So starting from about here, right when I start investing or sorry, when I start investing my money over a thousand months, 10,000 months, 100,000 a month and so on, it looks like we can kind of see a pattern again for a thousand months. It takes the program takes about five times, turns negative, 5 seconds to run. If I increase the number of months by ten, it takes five time to ten to the negative 4 seconds to run. And then as my input increases by ten, the number of months, my time to run seems to increase by ten as well. Right. So a .05.0. 5.7.8, something like that. Okay. So this is from a previous run. Of course, each run will be different because we're just calculating. We're just purely grabbing the time that the program took to run. So the actual time will be different, but few things to notice. So Python actually reported the time it took the program to run in scientific notation. Just kind of cool. So this is 4.3 times ten to the negative six. So it knew how to show it to me like that. So it's not doesn't have a bunch of zeros in there. And then the observation, as we might have, you know, as you might have guessed, is for this convert to kilometers was independent. Right. So this is the kilometers, not the compound function. But then the compound function here. This is again from a previous run. We can make a few observations. So the first was that the time only change only actually changed with the with the input when we changed end months, right? When we changed the initial investment or the interest rate. The program just basically took the same amount of time to run. So it was only end months that actually made a difference for us. Second observation, again, something we noticed is as we increase the number of months by ten, the time it takes the program to run also increases by ten. Again, something we've we talked about and the last observation. Is that we have this relationship very apparent as the input is really big, right? As the input a small I think I mentioned this last time, if for some reason my computer updates or decides to dedicate some time, some resources to do it to running an app in the background for whatever reason as it's trying to figure out the compound function with an input of one, this number could be changed dramatically, right? Because two times something negative six can be affected a lot by just a little bit of time dedicated to something else. Whereas, you know, 4 seconds or 14 seconds, if my computer dedicates a little bit of time to something else, that 414 won't be affected as much. Right? So when the numbers are big, that's when we can see that the behavior of our function a lot more clearly. Right? Not when the numbers are small. Okay. So. Now I'd like to look at some more functions. These functions are going to have the input being a list as opposed to just numbers. Right. We've seen a bunch of examples with numbers, but let's see what happens when my input is a list. So here's a very simple function. It takes in a list l and it sums all of the elements in the list. So we've seen this a bunch of times already. We initialize a total to be zero. We iterate through each element in our and we keep our running total by just adding the element to it to that total. Pretty simple. And we return it. Now, how do we actually run this function with a whole bunch of different inputs? Well, that's what we're going to see next. So this bit here is exactly the same as before. It's actually creating for us the list of 110, 100, 1000, 10,000 and so on. But clearly the number ten cannot be an input to this function. Right. Because this function is expecting a list. Right. So l cannot be ten. It needs to be a list with some things in it. So instead, what we're going to do out of that number 110, 100, a thousand and so on, we're actually going to make a list with just some random dummy numbers in it. I don't actually care what these numbers are, so let's just make them be the numbers. 0290 290 902 999. And in a fact, each one of these lists will then have one element in it ten elements in it, 100 elements in it, a thousand elements in it, and so on. All right, everyone okay with that? Right. So the input is now different. It needs to be a list. We're just creating a bunch of lists of varying lengths. So the relationship between these lengths are that the lists are ten times as big as the previous list. Right. Okay. So then now I have my input list here. I do the exact same thing as before. Not yet. I run the performance counter to create my starting time. I run my function and I get the data and I report the exact same thing as before. All right, so let's run that. Down here. Right. Running, running, running again. We have to wait a little bit. It looks like it's working, but it's just getting slower and slower, which is fine. So what do we notice? So when we had one element in my list, it took one time sending negative 5 seconds to run. When I had ten, it took one time ten. Negative 6 seconds to run. It was actually shorter to have more elements in it. See, this is what I mean when it's very unpredictable for low numbers. But as we get to lists of length, thousand, 10,000, 100,000, a million and so on, we can start to see the pattern. So with 10,000, it took four times ten negative 4 seconds to run with 100,000. It took four time the 3 seconds to run. And then as the input increases by ten, that is the length of my list increases by ten. It looks like the program takes ten times as long to run. A very similar thing as before. So the first observation that we can make out of this, this this function is that the size of the input, obviously, is now the length of our list. Right? It's not just the number ten or the number of thousand. It's a length of length. A list of length. Ten or a list of length. A thousand. Second observation is that, again, just like in the previous case, the average time increases by ten as the length of our list increases by ten. Okay. Very good. And just like before, this relationship between the size and time is more predictable for large sizes than it was for small sizes. Right. As we just saw here, which surprised me a little bit as well, is a for us a list that's longer it took a shorter amount of time to run. Just counterintuitive. But again, that's probably because my computer did something here to take a little bit longer to run. Okay. And then the last observation, this is this is compared to the compound function where we change the number of months. It looks like the time that this program actually takes to run is pretty comparable just in terms of pure, you know, seconds is comparable to how long it took for the compound to run. Right. So here when my input was 100 million, I think. Yeah, 100 million. It took about 8 seconds to run. And when my list had 100 million elements in it, it took about 7 seconds to run. Right. And then ten times faster going when we decreased our input by ten. Okay. So already we're starting to see something. Something that we're going to get. Yeah, go ahead. Oh, some of it's just this function that I wrote here. Yep. That's just. Did I name it something differently in here? No. Yeah. So already we're starting to get at this idea where I have two functions that do wildly different things, right? One, some of the elements in the list. The other one just loops over some number of months and does some calculation. But it looks like they're sort of in terms of just algorithmically wise, they are very similar. They take similar amounts of time. They increase at the same rate. And basically they just have a for loop, right. Or some sort of loop that iterates through the input and does something so algorithmically. We want to consider both of these functions the same, even though they implement completely different things. Other questions before we go on to another list function. Okay. So let's look at a slightly different problem. Dealing with lists. So this function or these three functions deal with finding an element in the list, and we're going to compare the runtimes of these three functions. So the first function is going to be a very brute forcing method to find the element in a list. It's got. Was there a question? Yeah. Yeah. Oh, my God. All right. No worries. Okay, so the first function will do a brute force search to find an element X. Right here. One of my parameters within a list. The other one of my parameters, basically given a list of a bunch of elements. This function, we'll just painstakingly look at each element one at a time and ask whether that element is the one that I'm looking for. Starting over here, the beginning of my list and going to the end of my list. That's what is in does. The next one. Binary search also looks for an element in a list, making sure that the list is ordered. But the way that it's going to do it is in a slightly smarter way. So I'm actually going to draw my lists this way. So each one of these is going to be elements in my list. So this is going to do a bisexual section search to find the element in the list. So again, we're looking for element X within this list L. And remember by section search. We start with a lot with a beginning and point and an ending end point. And our first guess for where? For the element or whatever we're looking for is to just say what's the is it the middle element? Right. So look at the element in the middle and ask, Are you the one I'm looking for? In this particular case, you look at the element in the middle and you say, Are you the X I'm looking for? Right. Good. So that's this one right here. So the midpoint calculation is right here. Right. The reason why we're doing slash slash for integer division is in the case where I have a list where I would actually, you know, look at the midpoint here. Right. Obviously, I can't ask the I ask Python for the element and index 3.5. Right. It doesn't work like that. So I'm just going to round down. You could also round up if you wanted to. I just made the choice to round down just so I'm actually grabbing the element at a an integer index. Right. So I've grabbed my middle element and then I ask, Are you the one I'm looking for? And if not, I ask whether this one is too low or too high. And then if it's too low, then I know I need to search this. Part of my list. And if it's too high, I need to search this part of my list. Right. So that's what this little if else is doing. Right. And when I make my decision as to which side to look at, then I reset my endpoints. And I do the process all over again by asking the midpoint there, are you the one I'm looking for? So on and so on. And so this does a bisexual search, also called binary search for the element in a list. And the last way for us to search whether an element is in the list is one that we've already been doing. It's this little one liner here is X in L, so using the keyword in. Right. So that's I call that the built in function, the built in operator in. Okay. So. It would be unfair if we just asked Python to figure out or to just pick a random number and ask whether that element was the one we're looking for. Okay. So instead, what we're going to do is to kind of take an average for each one of these three functions just to make it fair. So we're going to say when we're searching for an element in the list, I'm going to say that I'm going to take the average of the case when the element is the first one in the list. And I can find it maybe right away in this case, or maybe not right away in this case. The average with that plus when the element is the last one in the list. And plus, when the element is the middle one in my list. Right. So in that way, we're actually, you know, kind of covering all our bases, kind of best case, worst case medium case scenarios. Right. So each one of these three functions will be run with that. In mind. So that's these three functions. So this is my brute force is in this is my binary search. And the in it's obviously I'm just going to type in and when I run it. So I'll just show you for one of them. So we're going to uncomment this entire bit here. And run it. But you can see here. So instead of just running the performance counter and making one function call, I'm actually going to run three function calls, iterate it over this little loop where I'm looking for the element at the zeroth location, the element at the halfway location, and the element at the end of my list. And I'm just averaging those down here the time it takes to find those three. Does that make sense? Okay. All right. So this is a lot to look at. Luckily I'm going to summarize it in the next few slides so we don't have to stare at that at that python screen there. Okay. So we have three functions to run. Let's first look at how each of these three functions did individually, and then we can start comparing them to each other. So the first function we ran was the is in. So the remember this was the brute force one. We're painstakingly going through each element and asking if it's the one we're looking for. No, no. Smart way about that. Just brute force your way through. We notice that as the input list grows by ten, the time it takes to find the element in the list, whether it's the first one, last one, or in the middle, on average, also grows by ten. Okay. All right. Next. Let's look at the built in function. Well, worry about the binary one later. The built in function. So just using the in operator and you see this was down here. So this in. So basically the function I'm running is purely just asking whether X is in L right? And that returns true or false. So I didn't need to make a function for that. But that built in an operator also has a very similar trend, right, as the length of my list increases by ten. The time it takes for my program to run is also ten times as long. Right. I went .05 to point five and the next one would be five and so on. Right. Okay. So those seem to be doing approximately the same sort of they have the same performance. Now, what about the bisexual search or binary search? Well, this one is not so clear, right? If we look at the input, right. The input clearly increases by ten. From here to here. The time increases nine times and the -6 to 1.1 times and negative five. Right. And so the factor, how many more times it took is very unclear. Right. It's not quite one. If it was one that meant it's independent, right? It's constant. It doesn't matter what the input size is. It's always going to give us this amount. It's always going to run in this amount of time. So it's not quite one. So it's almost independent of size, but it's not linear, right? Like the other two functions were. Right. It's not ten when the input grows by ten. So we're not quite sure what this function is. Right, but clearly it's not as bad as the other two, but not as good as no relation. Okay. Observation from observation three observation for we'll now compare the function that we wrote the is in this one here to the binary search this one down here. Well. Binary search was orders of magnitude faster than brute force. Right. Brute force when the input was, what is this, 10 million or a hundred million? I'm not sure when the input was 100 million. Brute force took 1.6 seconds. But the bisexual search, the binary search took .0000 1 seconds. So it's not like we went from 1.5 seconds to 1.2 seconds or 2.5 seconds. We were orders of magnitude faster, right? Ten to the negative five. Right. So there's a really big difference between this algorithm, the one that's brute forces its way through and between this algorithm that does something smart about removing half of the search space with each each loop. Right. All right. So that's important to know. And lastly, just kind of comparing pure time that it takes these programs to run. Let's compare the function that we wrote, the one that loops one at a time through this list and the built in in function. The built in in function while it's still the same sort of has the same relationship linear right with the input size it seems to do a lot better consistently by about 10 seconds. Sorry, ten times as fast. Right. So when our function took point 1 seconds, the built in 1.0 5 seconds where when our function took one second, the built in function took 0.5 seconds. So consistently it's just faster to use the built in function than to make our own. All right. Questions about any of these observations. That makes sense. Are they interesting? Okay. So what do we see? Just a quick recap of those three functions, right. The first one we saw was linear in the size of the arguments. So when the input list size increases by ten, the program takes ten times as long to run. But this other one is something less than linear, but not constant. So we're not quite sure what it is. All right. We'll come back to this in a little bit. We'll end up plotting some of these runtimes. So we'll actually be able to see the relationship in a few slides. The next thing I actually want to do is do one more sort of function. This one is called the diameter. All right. And I'll explain what it's doing because it looks a little bit scary. But suppose we actually have some points in a 2D plate. Right. So it looks like this. Basically, what this function is going to do is it's going to figure out the biggest distance between all of these points. So, you know, the distance between these two points is, you know, something? This is the distance between these two points is something else. Which two points yield the biggest distance? That's what this function aims to tell us. And what that distance is. So the way it works is it has nested for loops. So this is different than what we've seen so far, right. We saw an example of this last lecture, but now we're seeing it in the context of something actually useful. So in this particular case, we're going to create an input list. All right. So. Again, our input list will just have some dummy values in it. I don't actually care what these numbers are. I just want to populate a whole bunch of points in a 2D plane. So what we're going to do is pretty much just iterate from numbers 0 to 10, sorry, 029 0 to 99, 0 to 999 and so on, just like we did before. And to get us a little coordinate. In the two D plane. Based on those numbers, I'm just going to take the cosine of that number, comma, the sign of that number, so that together. So, like, cosine of, you know. One or whatever sign of one will be this point here. Right. Set up as a tuple, and then this one might be cosine of five right karma sine of five, something like that. So I'm just making a whole bunch of coordinates in a 2D plane, ensuring that I have an coordinates. Okay. Now the loop. Ah. Sorry, there's going to be two loops. The outer loop will basically take us through each of these elements. Okay. I have five in this particular case. And the inner loop will go through every other element. Right. But notice it starts from I plus one. And I'll tell you why that is instead of starting from zero. So let's just walk through. Let's say we start out with this element, this little as our first element in our outer loop. So right now we've grabbed the first element in our our outer for loop. And what we're going to do is figure out the distance between it and everybody else. So now we're iterating through the inner four loop, going through each element except for myself. So I'm going to get the distance between this one and this one since it's the first one, obviously it's the the biggest one. But then I'm going to get the distance between this one and this one. Right. And I'm going to say, are you bigger than this one? It looks like no. So we're still keeping this one as our longest one. Then I'm going to grab the distance between this and this one and this and this one. And as I'm going through this little if statement here keeps track of the farther one. So the one that has the biggest magnitude right in this case, that's probably the first one we looked at. And after I've gone through each element, each other element, I've concluded my first iteration in my outer form loop. So now. The outer four loop goes to the next element in the list. Let's say it's this one. It doesn't actually matter. Okay. This one. We'll look at the distance between itself and everybody else, except for the one we already looked at, because we already know this distance. Right. We kept track of it already when we iterated through this one. So as I'm going through my outer loop, keeping track of this this point here, it figures out the distance between this one. This one, which is suddenly bigger than that one, which we had to keep track of. And then this one right here. All right, good. So now we're still keeping track of the biggest distance we've seen. It's probably this one here, and I've concluded the second iteration of my outer for loop, and now I go to the next element. Let's say it's this one doesn't matter again. Now this one is going to get the distance between itself and everybody else except for the two that we've already seen, this one and this one. Right. So that's why our inner loop starts at our plus one. So this one will get the distance between itself and this one all the way back there and this one all the way over here. Right. And then next iteration in the outer loop takes a look at this one, let's say, and it finds the distance between itself and everybody else. But you know what? There's only one left on there. And then the last time through, this one doesn't even get a chance to find the distance between itself and anybody else, because everybody else already found the distance between it. Okay. And so on this way we're basically finding all the possible pairs of all of these points in this 2D plane and keeping track of the longest of the biggest distance. So in terms of the list, the input list, the way that looks like this, I plus one business here, the outer loop basically says, I'm going to start with you and I'm going to get the difference between you and the element at index one, the element at index two and the element at index three. This outer for loop is done. Next, we're going to get the difference between the distance between this one and everybody else. Right. So obviously not the element index here because we already know that distance. So we're going to get the distance between element and index one and index two and index three. And then we're done. And then the last loop, outer loop, gets the distance between element and index two and index three. And then it's done. Right. So just these two nested loops just does all of this. Until it finds the always Paris pairs basically pairs up everybody together. Okay. So if we run it. What are we going to see? So my input for this particular function, you'll notice, first of all, is going to be much, much smaller than the inputs for everything we've done so far. Right. Some of the inputs we had seen in the past were a million, 10 million, 100 million. In this particular case, I'm only going to go up to 6000 because it's just going to take way too long to run. If I make it go for any much any longer than that. So what do we see already? We've got 100 points or 100 of these, right? Finding the distance, but finding the maximum distance between a bunch of these pairs took about 0.0 3 seconds. If we doubled that to 200 points, it took 0.10 1 seconds. If we doubled that to 400 points. It took 0.0 5 seconds and so on and so on. So just like before. Let's take a look at the big numbers to see our trend. So as the numbers increase right by two, if my input increases by two, it looks like the time that it takes for me to find out the biggest distance increases by four. So my input increases by two, the time increases by four. And I'm not going to run this, but you can make a new list on your own and change this to be inputs that are multiple that are multiples of ten. Right. Increasing by ten each time. And you'll see a very similar pattern where the where the time it takes to run that program will be about 100 times as slow. So the relationship there is a and an squared kind of relationship. All right. So a few observations here as well. First one I already mentioned is this program just on just takes a lot longer to run in general. Right. So here we were able with our compound and with finding whether an element is in the list and getting the sum of all the elements in the list, we were able to run 100 million a list with 100 million elements and it still took about one one something seconds. Whereas with this diameter function we can barely get to 6000 and it's already taking 14 seconds. So just way, way, way slower program in general. Right. And then the relationship seems to be a an and squared kind of relationship relating the input to how long the program takes. So let's actually plot well, I already did this, but here are the relationships for these sort of three types of algorithms that we've seen so far. So the this is the finding the element in a list those three versions sorry those two versions that we saw. And this is the diameter function. So if we plot how long it takes the program to run when the input increases, sorry, when the input is this size, we can see that there is a linear relationship. So the time it takes for the program to run is linear in the input. The diameter. We'll talk about the binary search and the diameter. We all going to notice this just by looking at the pure numbers, but it's a lot easier to see it visually when the this is on the x axis the size of the problem. So how many points we actually are finding the diameter between and how long it actually takes the program to run again? The relationship is quadratic. Now that we plot it, we clearly see the quadratic relationship. And then this binary search, we were very unsure of what it was. Right. It wasn't quite constant. It definitely wasn't linear. But now that we've plotted it, so this is the input size and this is how long it actually takes the program to run. You can see it drastically increases when the input size is very small, but then it kind of sort of asymptotically reaches some sort of value. It's actually a logarithmic relationship. All right. Last thing I wanted to mention about timing before we move on to counting is just pure running. Just purely running these functions on different computers will just give us different values just right off the bat. So for my on this newer ish computer, how long did it take to run this compound? Well, you know what? It took like 3 seconds or something, one point something seconds on an older laptop. It took, you know, 663 seconds on an even older desktop. It took 1226 seconds. Right. So just for just purely timing things, the machine you're running it on is going to make a difference. Okay. And then that's fine. Right. It's important to know how long it takes. But if you're just looking at the relationship between input and how long the program takes to run, that's the same. So it doesn't matter what machine you're running it on. When you increase the input by ten, the program will take ten times as long to run. No matter whether you're running on a fast laptop, old laptop or super or desktop. So just timing a program is really important. Right. You'd like to know whether the program you wrote, you're going to have to wait, you know, a month for it to finish or a couple of minutes for it to finish. That's an important thing to know. But what we're going to get at towards the end of this lecture is something that's complimentary and that's this idea of asymptotic complexity, just kind of mathematically saying, you know what, this program is not going to be that bad to run. Right. You're not going to have to wait for it for months to run without actually running it, of course. So you be able to glance at a program and say this one is reasonable to run. And so we're going to do that in terms of this idea of order of growth, which we'll get to at in a little bit. Okay. Any questions on timing before we get to counting? Oh, yeah. Yes. The function. Yes. Can you assume that all building functions are optimal in terms of running time? Yes, certainly better than when we when we write them. Ah, yes. In Python. And then of course in other languages, you know, there would be there may be take advantage of other speed ups as well, like putting things in memory efficiently. But yeah, generally it's better to run something that's already been made than to make it yourself. Yeah. Okay. So now what we're going to do is we're going to count operations just like we did last lecture. Clearly, timing is nice, but it doesn't give us a nice relationship besides us like spotting it, right? There's no formula. There's no relationship. That relates the input to how long it takes the program to run. Counting will get us a little bit closer to that. And we saw that last lecture. Let me remind you the idea of counting. So. The idea of counting is that we're going to take a bunch of these operations like mathematical operations, comparisons, indexing into something and assigning a value to a variable. All of these things right when we run them, yes, they might run for different amounts of time, one time, something negative nine versus two times negative nine, something like that. But that's very that's not a very big difference. And so what we're going to say is that every one of these operations will consider to be constant. Right. They will take one unit of time. So if we say that we can actually come up with a relationship that tells us according to that, that relates the input to how much this how many operations this program will. So here in the convert to kilometers. What do we have? We have one multiplication. And you know, just for the heck of it, this lecture, let's say the return also counts as an operation. So in this convert to kilometer function, we have two operations. Notice that it's not really related to the input at all. So then the amount of operations that this program takes to run is always to. It matches what our timing said, right? It basically didn't matter what the input was. They always took approximately the same amount of time to run. The sum of function. So it takes in an input list and it gets the sum of all the elements. This one will do. We'll have one operation for doing this assignment. It'll have one operation for grabbing an on an element in my list l and assigning it to I. It'll have to operations for this total plus equals I remember total plus I on the right hand side is one operation and total equals. That is my second operation. So that's two operations. And then let's not forget our for loop. That's kind of the important part of this function. How many times will these three operations repeat this one plus these two? Well, it's going to repeat however many elements. I have an l so length l times. Okay. And then again, let's say we count the return. The return will also be one operation. So the total number of operations for the sum of function will be one for the total equals zero plus length of l times three, because there's three operations being done for each length of L plus another one for the return. So that's going to be three length L plus two. That's a nice little formula that relates how how how many units of time we'll have to wait depending on the size of the list. Right. That's pretty cool. So the way that we're going to count the number of operations, again, I'm going to do it slightly differently. The last lecture just to show you that there is another way to do it. So this is our function is in it's going to count how many operations we have and I'm going to use something called a global variable. And I'll show you again the difference between them. So it's just these three lines that I added. And you should never, ever use global variables in your programs except in this situation. The idea of global variables is that you can define variables just in the main program outside of any functions. And you can access those variables within some function purely by saying, you know, if we defined count out here right before this function definition count equals zero or whatever inside of any function we can say, Hey, Python, I would like to access this variable that I defined outside of this function. You say that you tell Python using Global and then the name of that variable and Python will grab that variable that's basically quote unquote shared across the entire program and modify that variable. Right. So in there, in essence, we're basically saying this is now a shared variable. If I modify it within this counter, within this function, it'll be obviously modified for everything else. It's very tempting to use global variables because, you know, all the variables you could ever want to create are going to be accessible by everybody. Right. No need to pass in parameters. No need to do. But it's very, very bad programing. So we won't ever do it, except for in this particular case, because we'd like to keep a counter of things that are happening or for debugging purposes and things like that. So the count variable will keep track of. It'll just increment in key places where we have these constant unit of times happening. So I've got count plus equals one here because I've got my return value. I've got count plus equals two here because I grab an element from L and I do the equality check here and then that's it. So if I run that in down here. What are we going to see? Well. I don't actually do how many how much more it ran. But we can see the the relationship, right? We go 9 to 30, 7 to 300 7 to 3000, 7 to 30007 and so on. So again, the same relationship where we increase the input by ten, the number of operations we do is ten times is more exactly like the formula said it would be. What about the binary search? So again, we're going to use this global variable and we're going to have the counter keep track of all of these operations. So this count incrementing by three accounts for setting the low to zero, setting the height of this thing and grabbing this actual value of length. Incrementing the count within this while loop will keep track of this subtraction as one operation and the test that is greater than one is another operation. Counting increasing by three here accounts for high plus low that integer division and assigning that value back to mid. Count plus three here accounts for indexing into this l. The less than or equal check. And then either doing this reassignment of low or this reassignment of high results, three operations and then lastly, count increases by three once more because I've got these operations here. So indexing into the into law, checking for equality and then doing the return. Okay. So the actual number of operations will be kept track of in right by the counter. So all we're doing is just kind of reporting how many times, how many operations we've done. So as we increase the input by ten, just like with timing, we can't quite tell what the relationship is. Right? Again, it's it's like one point something, right? With each run. All right. So this is these are the the results. So the observation one, as I mentioned, when we increase the input by ten, this brute force, I released it but the brute force is in function also is does ten times as many operations the binary search. Again we don't know what rate it is at, but we can plot them. So here I have the plots, just like when I plotted the input size versus how long the program actually took to run. I'm now plotting the input size versus actually just the number of operations being done. So the is in function. That brute force way of finding whether an element is on the list grows linearly. No surprise there. And how lucky for us the binary search matches the graph matches the one that we had for timing. Right. So as I increase my size. In the binary search method, the number of operations that I do is logarithmic and in time, just like we saw in the actual time. Okay. So. Timing and counting are really nice, right? Timing gives us pure number of seconds or months or whatever. We need to wait for this program to finish, but counting gives us a nice little formula, right? That relates the input to the number of of of operations that you have to do. You might have noticed I briefly touched upon this thought throughout this entire lecture and last lecture, we basically just saw something like, you know, three different algorithms. Right. We saw something that's constant, something that's linear, something that's, you know, quadratic and something that's binary, logarithmic in this particular case. Right. So that's four different algorithms. But we saw way more functions. Right. Right. So what we'd like to do is evaluate the algorithms, not the different implementations. Right. And what we'd like to do is evaluate these algorithms as the input gets really, really big. So what we're going to do is figure out a relationship between the programs runtime. And the input. But what we're going to do is focus on the biggest terms that contribute to the program's runtime. Right. So we saw these examples last time, right? This my sum, which basically summed all the elements, all the numbers from zero to X, and this the silly square function that had nested loops kind of like this diameter. Right. We were able to say something like, you know, when the import increases by ten, the program is ten times as long to run. Right. So the efficiency of that program was on the order of X when the input increased by X. The program took X times as long to run. The square had a similar we could have said in a similar way. Right. When the input increases by x, the program took x squared as long to run. So I don't actually care right about all of these differences in the exact time x one time send negative six 1.3 times 10 to -6. I don't care. What I do care is the order of growth. How does the program run in relation to the input? Okay. And I care about that when the input is really, really big. So what we're going to do is express the program's runtime in an order of not exact kind of relationship. So while counting was really nice, right? It told us a nice relationship between the input and the number of operations when the input is really, really big, like three x plus four when x is really, really big. I don't care that the number of operations is three x plus four, right? Because when x is really, really big, that plus four might as well be plus zero. And that three x is basically like X when the input is really, really big. Right. So that's what we're going to try to do. All right. Now, before we do that, we need to decide what to measure, right? Because when we write functions, we're going to have functions that have a whole bunch of inputs potentially. Right. So the input could be an integer like in convert to kilometers. It could be a list, in which case we would be interested in maybe the length of the list. And if you have many parameters, you'd have to decide, right, what is the parameter that contributes to the to the growth of this function? So here's an example. This is our is in function. It looks for an element E in list F right. So there's two parameters to this one. We can ask, does the program take longer to run as e increases? It's one of the parameters. Let's see what happens as we make it bigger so we can look at a little example. If we find out whether zero is in this list containing 1 to 3 or whether a thousand is in the list, one, two, three. Does the program take longer to run? No, exactly. So it is not really relevant in my run time sort of calculation. All right. Well, let's consider Elmo. When we say L is going to change. It could change in two ways, right? The elements in EL could have different values. Right. Or the list length itself could be different. So in this particular function, let's say that the elements in L are small numbers versus big numbers. Okay. That's certainly something that could happen and certainly with some functions that's going to make a difference. So let's say in this particular function, if the elements in L are big versus small, is it going to make an impact on my runtime? Well, here's a little example. Let's say I'm looking for the numbers. Zero inside a list with one, two, three, and the number zero inside a list with 1000. 2003 thousand. Is that going to make a difference? No. Right. So the size of the elements themselves don't really matter. And what less thing to ask ourselves is what about the length of the list? So if Elle has different lengths, will this be a difference in our run time? Right. So if I'm looking for zero on a list of three elements or zero on a list with ten elements, we're clearly that zero is nowhere to be found. Is that linked list that's going to have a difference? Yeah. In this case, it will. Exactly. So here in this particular function, the input I'd be interested in, in, in sort of reporting the runtime from is the length of the list, not the elements in the list, not itself, but the length of the list. Okay. So the last thing that I'll mention is for this particular class, we're going to talk about the worst case scenario. So you might have noticed in this previous example here. Right, I always looked for an element that wasn't even in the list. Right. So when you're faced with a function, you ask yourself this particular class at least what is the worst case scenario and finding out whether it functions in the list. The worst case scenario for us is if it's not in the list at all. Right. So that's sort of another aspect of runtime that we don't actually we won't talk about because for us we're always interested in the worst case. But there are certainly certainly analysis where you could look at the best case scenario, which is, well, the element is the first one on the list. Right. In that case, you're always going to find it right away. So it's constant or an average case scenario, which is kind of what people do in the real world. Right? You're not always encountering the worst case. But for us, we're going to look at the worst case scenario. So our goal is going to be to describe how the runtime grows as the size of the input grows in a really general way. So we're not going to be interested in figuring out the exact number of operations, No. Three X plus two kind of deal here. We're just going to focus on terms that are really that grow the fastest. We're going to eliminate any sort of additive multiplicative constants and things like that. So we're just going to focus on terms that grow the fastest and that will give us our order of growth. So the way we're going to denote the order of growth is using this notation called Big O and Big Theta. Okay. Now. Morning. We're going to have some math coming our way. It's going to be like three slides of just pure math. Okay. You can sit back. You won't need to know it. No need to know the details. But it will motivate us to kind of give us the idea about this asymptotic order of growth. All right. So this is the the mathematical definition of big. So what we would like to do, there will be a drawing board. So what we would like to do is figure out an upper bound for our function. So the function might look like this. And I know this is just an F, but we relate this to our class by saying, you know what, if we did the order of calculation, sorry, the number of operations analysis, right, for a function, we could basically come up with something like this, right? We came up with three x plus two. We could come up with three x squared plus 20 x plus one for some random function that we wrote. Right. So that we considered the function. Now the big O is going to be the upper bound on this function. So if I plot this function in my in my x y axis, this is what it looks like. The big goal will be some other function that's going to upper bound. This one, the blue one. Okay. And it's going to upper bound it for all values beyond some X. Right. So for all values beyond some X, some some number on the x axis, some crossover point, this big of g, this g will always be bigger than my F. That's the idea here. So clearly X is not going to upper bound it, right? Because after this crossover point, X will be below my function no matter how big, how big of a constant. I tack onto that x, I could have a thousand x that's still not going to upper bound. My little blue line here. So what we're going to do is we're going to increase the the exponential there. So let's take x squared. Well, X squared is getting closer. It looks like they're both quadratics, but this orange line is not above the blue line for some crossover point. Two. X getting closer. Three X Getting closer. Four x. Is an upper bound on my f. Because after this little crossover point here at about 20, my orange line, the G will be always above my blue line, my F right. So far so good. Just visually speaking. Yeah. There. It that. Yeah. The orange one is below it. That's totally fine, because what we're interested in is the behavior when the input is really big. So that's why I don't care about, like, weird stuff happening down there. All I care about is when. When my excess super big. Okay. So now I found this. So I can say that after this point 20, my orange line will always be above my blue. So I can say that my f is big. All of g is big o of x squared. Okay. Because I don't care about this force so much because it's just a multiplicative constant, because this for x is always greater than my function for all x greater than this crossover point here. That's it. That's the definition. Right. So the G here is basically this function without the multiplicative constant in front of it. Okay. So I say three x squared plus 20 x plus one is big of x squared. So generally speaking, that was just an example. Generally speaking, the big low is an upper bound on my function and this is now just using variables like constants and things like that. But it's exactly the same situation that we had from before. Okay. So I'm going to try to map the blue to the blue and the orange to the orange and the purple to the purple to help you kind of match up what we saw in the previous slide. So basically, we say that our function F is big, all of this orange G, right? If we can find some blue constant, right. Where this constant was this for here, where that constant multiplied by g for x squared is greater than my function for all our values beyond that crossover point. Right. So I found my four because four x squared is always greater than my function beyond 21. Right. That's what we saw in the picture. Right. So then we can say that my function F is G is big O of g of x, that g is x squared just matched. All right. So in terms of the picture here, right, this is kind of a little zoom in of what happens. Anything can happen down here. But beyond the crossover point, which is here in the big picture, that crossover point, beyond that crossover point, my orange is always greater than my blue. So what does this mean? We're going to talk about this in a few slides, but you might have thought about this. I can actually pick any function that grows faster than what is this, three x squared, right? I can pick x factorial. X factorial grows super fast or two to the x that also grows super fast. All of those functions that grow way faster than mine are also upper bounds on this on this function. Okay. So that's big. So it's just an upper bound. Then what is theta? For the reason I just stated. Right. I said x squared sorry. X factorial two to the x. All of these functions that grow much faster than my function are all upper bounds. And that's not really helpful for us when we say, Oh, this function is big, all of whatever, right? Because you can just pick something that's ludicrously fast, that grows ludicrously fast and say that has no meaning. So instead what we usually report is the theta, which is actually an upper bound and a lower bound for our function. Right. So using the exact same reasoning, we're going to find some constant tacked on to that g of x such that that function grows is always underneath our function. Okay. So again, I'll put up a lot of math, but basically these first two lines here, this one here, there exists blah, blah, blah, blah, blah. That first here this is is the big O definition. So we've already know what that means. All we're going to do is tack on another condition, which is that we can find another constant for that same g. Where that function beyond some crossover point is always below my blue, my blue light. So here's an example. Right. For X squared, we saw that it grew faster than three x squared beyond a crossover point. Well, we could say two X squared will always grow slower than its own crossover point. Okay. So the constant for was the same as we had seen before, but this constant to now becomes a lower bound. Right. So I'm basically trying to have that same g, both upper bound and lower bound. My blue function. Right. And that's the definition of data. So now I can no longer say that to to the right an exponential both upper bounds and lower bounds it because that to to the X will grow faster than my function no matter what constant I tacked attack on to it. So now what we see is that really the g of x is going to be the term that grows the fastest. It's just going to be that term here. Right. It's going to be the thing without the fastest growing term in my function, without the constant on it. Okay. So, yes, we will never remember all that, but we're going to do a bunch of exercises and you're going to see just how easy it is to figure out the order of girls. Okay. But I will mention this just again, because it's very important. Right. So when we're talking about upper bounds, you can pick any function that grows faster than yours. Right. F of x, this three x squared thing is all of x squared. Yes. But it's also over x cubed over x to the five of x to the 100 over two to the x of x factorial. All of those things that grow much faster. But my f of x is only one theta and it's theta of x squared. Right. And that's the term that grows. That's the term that grows the fastest in my function here. So when we look at a function, right, based on the number of operations or however you know, you know, you're given the function, when we look at the order of growth of the function, we just focus on the dominant term. Right. So in the first one, the input here is N and the function is and squared plus two and plus two. Which one of these is the dominant term? You tell me. Yes, exactly. And squared. So this function is just going to be theta then squared. That's it. How about in the next one? What's the dominant term here? Yeah, exactly three x squared, even though 100,000 x is going to be huge for a while and this constant is also going to be huge for a while as X gets really, really big, this three x squared and in fact just x squared will kind of take over everything else, right? So this next one is all theta of x squared. How about the next one? What's the term that grows the fastest here? Yeah, exactly. Right. Log is. Is, is is. Right. So this theta of this function is just theta of it. So notice what we're doing here is just focusing on the dominant term. We're going to drop the multiplicative constants, drop every other term and relate the theta in terms of the input. Right? So I don't just use theta of an all the time. Right. In the previous one, it's tempting to say the first one, state of an squared, the next one state of N Squared, the last one state of n. But N is not always the input to your function, right? If it is great. If it's not, you always have to relate it according to the input of the function. Maybe it's length l. Maybe it's something else. Okay. So let's have you try a couple more. What is the theta of the first one here? That's the term that grows the fastest. Yeah. Fate of X. Next one. And cute, but exactly what I told you is going to be so easy. I know that math is scary. How about the next one? That's the term that grows the fastest, but then it's theta of dropping in multiplicative constants and it's just theta of y. The last one is going to be tricky. What is the theta? If the if the variable is only b. Yeah. To to the beat. What about if the variable is only a. AQ Exactly. And if my function is both a function of A and B. And a plus two plus eight cubed. Right. Because both will contribute to the runtime of this function. Right. Not just the B, right. So if this function, whatever this crazy function is that I wrote that takes so long to run was had both inputs B and a right as its parameters. The theta for that function is both is in terms of both BNP the dominant terms of each. Yeah. What are you up to? No. No need to worry about negative coefficients. Overreact to a ton of negative. Yeah. Yeah. Yeah. Question. I guess. Big data presented to us these days. Oh, some different parameter. The variable that's not even here. Yeah. If the, if we were, if the parameter, the function was C let's say for this last one, but the formula was this then the theta would be just constant theta of one. Because it doesn't even depend on these variables. So these are just considered Constantine. That's a great question. Yeah. If the parameter was C or something else. Okay. So now we can actually look at functions that we write and we do the exact same thing. We can first start out with just saying, how many operations does this function take come up with that relationship and just theta that right, just like we did on the previous slide. So here's a function that that calculates stuff a factorial. What do we have here? Well, we've got this is constant here. Right. We've got just one wild loop where there's five things going on here. There's the comparison. There's this. Times equals, which is two operations. This minus equals its two operations. So this function is just five and plus two by the same analysis. We did, you know, a few slides ago. Right. So if we say, what's the state of this function? Well, what's the theta of this thing? Five and plus two. Super easy, right? It's just the state of it. And in this case, the paramount parameter of our function is trillion. When we have functions that are slightly more complex and we've got things that are in series, like for example, here I've got two for loops, one right after the other. We basically use this law of addition to take care of that. So that means we figured out what the theta is for the first four loop, the theta for the next for loop, and we just add those to fade us together. Okay. So the first for loop here is state of MN because it's something that depends on parameter MN. And the next for loop here is theta and squared. Right. And this, this because the parameter here is end times. The stuff inside the for loops are constant, so they don't contribute anything to our theta. Right. There's no more things to multiply the complexity there. So the if this is my entire function here, the theta for this function is theta then plus theta of n squared. Right. And the law of addition just says theta event squared is just theta of sticking those two inside as as part of my function and plus and squared and we know how to do that. That just simplifies to the dominant term which is n squared. Okay. So that's the law of addiction. So that's when we have loops or things like that in the series. What about when we have loops that are nested? Right? Then we use the law of multiplication. Because for each one of these things, we're going to have to do this that many times, right? So in this particular case, we need to be careful. The outer for loop is going to be theta even. And the inner for loop. Is also theta even. Even though I'm dividing by two. Right. 0.5 times n is still theta. Right. That multiplicative constant in front of that n is 0.5. Which is just, you know, it's just it still leads me to be theta of it. The print is constant, so there's nothing else to multiply there. So the law of multiplication just says that often times theta event is theta of and squared inside there. Okay. So let's look at this program. What is the theta for this? Well, we can do it sort of in very grave detail. We've got X as our parameter, so we only count loops and things like that that are a function of X. If I had a loop that was a function of I don't know, and or something that doesn't count because it's not a function of my input. So only look at things that are a function of x. I've got one outer for loop that goes through X times. So that's state of x. I've got an inner for loop that starts from AI and goes to X. That's a little bit tricky, but in the end, overall, it's going to be fate of X because it's going to be the first time it's going to go through X times. The next time it's going to go x minus one, then x minus two, then x minus three. So we're effectively just kind of adding over all of these runs, X plus oh, sorry. One plus two plus three plus four plus five all the way up to X. And that's just some function of X, right? It's definitely not going to be constant. So the inner loop is also theta of x. Everything else is state of one. There's nothing else that depends on. So this whole function is going to be theta of one for this assignment here, theta of x times theta x for this nested loop here and theta of one for this return down here. So overall, it's just going to be theta of. Right. So that's. And so overall, it's just going to be theta of X squared just by the laws of multiplication and addition. All right. Think about this and then tell me what you think it is. What do you guys think it is? Yeah. Fate of. Length of. Absolutely right. So this is constant. This stuff inside the loop is constant. The return is constant. The only thing that depends on L is the length of the last right. This loop. So the answer is theta length. Perfect. How about this one? So here we're assuming the link. All the inputs are the same length. Yeah. Fate of length of pick your favorite one side of length. L was reasonable. You could also say theta length l one or theta fell two because these are two loops that are in series, right? So this one just loops through the length of L. But inside we're not doing anything that costs more than just constant time. Right here, we're just comparing two numbers like three and two. We're just assigning something to true. So nothing else really depends on the, the length of the list. So this is theta of length. So this is plus theta length. So that's just theta like. All right. So we saw a bunch of different algorithms. Right. Sorry. We didn't see a bunch. We saw a bunch of different programs, but we could kind of classify them all into one of these categories. Right. And this is all basically all the different algorithms that you could ever write in general. Right. So something that's constant theta one, something that's logarithmic is theta log in, something that's linear. We saw many of these is theta, then something that's log linear. We haven't seen any yet, but that's theta and log in theta. And to sum constant like and squared and cubed is polynomial and theta of some constant to the end like two to the n, three to the n is exponential. And when we're writing our programs, you can do a quick analysis of the of the program that you just wrote. Look at the loops, look at to see how efficient you wrote it. And you can basically classify your program into one of these categories. Right. If you had nested loops that both depend on the input, you probably wrote a polynomial type algorithm. If you just had one loop that depended on the input, you probably wrote a linear time algorithm. Right. And when we write these algorithms at a first pass, we want to be somewhere up here. You don't want to do anything that's x polynomial or definitely not exponential because things get slow really quickly with those numbers. Right. And so we never, ever want to be in that situation. Sometimes it's unavoidable. Right. So that's all I've got. Next lecture, we will be going through a bunch of those different complexity classes and looking at different programs that land in those classes. Right. Especially the the logarithmic and the log letters. All right. 
