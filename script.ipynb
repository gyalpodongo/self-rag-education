{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex,SimpleDirectoryReader,ServiceContext\n",
    "from dotenv import load_dotenv\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.llms import ChatMessage\n",
    "from llama_index.embeddings.langchain import LangchainEmbedding\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "import os\n",
    "from llama_index.core.prompts import PromptTemplate\n",
    "from lecture_formatting import lecture_dates6100L\n",
    "from pydantic import BaseModel\n",
    "from typing_extensions import Literal\n",
    "import json\n",
    "import torch\n",
    "import random\n",
    "from datetime import datetime, timedelta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IsSeen(BaseModel):\n",
    "    \"\"\"An object determining if a document has been seen or not\"\"\"\n",
    "    seen: bool\n",
    "class IsSup(BaseModel):\n",
    "    \"\"\"An object determining if a document fully support, partially supprt or none support for an answer\"\"\"\n",
    "    sup: Literal[\"fully\", \"partial\", \"none\"]\n",
    "    \n",
    "class IsUse(BaseModel):\n",
    "    \"\"\"An object determining how useful an answer is from 1 to 5\"\"\"\n",
    "    use: Literal[\"1\", \"2\", \"3\", \"4\", \"5\"]\n",
    "\n",
    "class IsRel(BaseModel):\n",
    "    \"\"\"An object determine if the retrieved passage is relevant to the question\"\"\"\n",
    "    rel: bool\n",
    "\n",
    "class IsRet(BaseModel):\n",
    "    \"\"\"An object determining whether a question requires retrieval of external documents\"\"\"\n",
    "    ret: bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_llm(model):\n",
    "    \"\"\"\n",
    "    Function that return LLMs given model and extra arguments\n",
    "    TODO: Add more LLMs\n",
    "    \"\"\"\n",
    "    return OpenAI(model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(seen_obj, rel_obj, sup_obj, use_obj):\n",
    "    w = [0.2, 0.3, 0.2, 0.4]\n",
    "    seen = 1 if seen_obj.seen else 0\n",
    "    rel = 1 if rel_obj.rel else 0\n",
    "    sup = 0\n",
    "    if sup_obj.sup == \"fully\":\n",
    "        sup = 1\n",
    "    elif sup_obj.sup == \"partial\":\n",
    "        sup = 0.5\n",
    "    use = int(use_obj.use)\n",
    "    return w[0]*seen + w[1]*rel + w[2]*sup + w[3]*use \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gyalpoaguirre/Dropbox/Mac/Documents/mit/6.s893/self-rag-education/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/var/folders/q_/gmxdkf893w3bm9wxvh6635t80000gp/T/ipykernel_53460/2349502471.py:4: DeprecationWarning: Call to deprecated class method from_defaults. (ServiceContext is deprecated, please use `llama_index.settings.Settings` instead.) -- Deprecated since version 0.10.0.\n",
      "  service_context=ServiceContext.from_defaults(\n"
     ]
    }
   ],
   "source": [
    "embed_model=LangchainEmbedding(HuggingFaceEmbeddings(model_name=\"mixedbread-ai/mxbai-embed-large-v1\"))\n",
    "llm = get_llm(\"gpt-4\")\n",
    "\n",
    "service_context=ServiceContext.from_defaults(\n",
    "    chunk_size=1024,\n",
    "    llm=llm,\n",
    "    embed_model=embed_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_distance(tensor1, tensor2):\n",
    "    \"\"\"\n",
    "    Calculates the cosine distance between two tensors.\n",
    "\n",
    "    Args:\n",
    "        tensor1 (torch.Tensor): The first tensor.\n",
    "        tensor2 (torch.Tensor): The second tensor.\n",
    "\n",
    "    Returns:\n",
    "        float: The cosine distance between the two tensors.\n",
    "    \"\"\"\n",
    "    dot_product = torch.dot(tensor1, tensor2)\n",
    "    norm1 = torch.norm(tensor1)\n",
    "    norm2 = torch.norm(tensor2)\n",
    "    cosine_sim = dot_product / (norm1 * norm2)\n",
    "    cosine_distance = 1 - cosine_sim\n",
    "    return float(cosine_distance)\n",
    "\n",
    "def embed(text):\n",
    "    \"Returns embedding of a text in the form of a tensor\"\n",
    "    return torch.Tensor(embed_model.get_text_embedding(text))\n",
    "\n",
    "def generate_random_date():\n",
    "    start_date = datetime(2022, 9, 7)\n",
    "    end_date = datetime(2022, 12, 15)\n",
    "    \n",
    "    time_between_dates = end_date - start_date\n",
    "    days_between_dates = time_between_dates.days\n",
    "    \n",
    "    random_number_of_days = random.randrange(days_between_dates)\n",
    "    random_date = start_date + timedelta(days=random_number_of_days)\n",
    "    \n",
    "    return random_date.strftime(\"%m/%d/%Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents=SimpleDirectoryReader(\"./formatted_lectures\").load_data()\n",
    "index=VectorStoreIndex.from_documents(documents,service_context=service_context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = index.as_retriever(similarity_top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selfrag_response(question, student_date):\n",
    "    response = \"\"\n",
    "    prompt = \"You are a teaching assistant helping a student answer different questions.\"\n",
    "    retrieval_tmpl = PromptTemplate(\"Generate a Retrieval object given this question {question}\")\n",
    "    ret_obj = llm.structured_predict(IsRet, retrieval_tmpl, question=question)\n",
    "    if ret_obj.ret:\n",
    "        retrieved_nodes = retriever.retrieve(question) \n",
    "        seen_tmpl = PromptTemplate(\"Generate an IsSeen object given the fact that the student has seen all lectures up to this date {student_date} and this current lecture has been given on this date {lecture_date}.\")\n",
    "        rel_tmpl = PromptTemplate(\"Generate an IsRel object given a passage:{passage} and question: {question} toetermine if this passage is relevant to this question\")\n",
    "        use_tmpl  = PromptTemplate(\"Generate an IsUse object by ranking how useful the response: {response} for this question: {question} is from 1 to 5\")\n",
    "        sup_tmpl = PromptTemplate(\"Generate an IsSup object by evaluating how supportive the response: {response} for this question:{question}, whether it is 'fully', 'partial' or 'none' support.\")\n",
    "        nodes = []\n",
    "        for node in retrieved_nodes:\n",
    "            lecture = node.node.metadata[\"file_name\"].split(\".md\")[0]\n",
    "            lecture_date = lecture_dates6100L[lecture]\n",
    "            seen_obj = llm.structured_predict(IsSeen, seen_tmpl, student_date=student_date, lecture_date=lecture_date)\n",
    "            passage = node.node.text\n",
    "            rel_obj = llm.structured_predict(IsRel, rel_tmpl, passage=passage, question=question)\n",
    "            temp_prompt = prompt + f\"\\nThe following is an excerpt which could be useful for answering this question: {passage}\"\n",
    "            messages = [\n",
    "                ChatMessage(role=\"system\", content=temp_prompt),\n",
    "                ChatMessage(role=\"user\", content=question),\n",
    "            ]\n",
    "            temp_response =  llm.chat(messages).message.content\n",
    "            use_obj = llm.structured_predict(IsUse, use_tmpl, response=temp_response, question=question)\n",
    "            sup_obj = llm.structured_predict(IsSup, sup_tmpl, response=temp_response, question=question)\n",
    "            rag_score = get_score(seen_obj, rel_obj, sup_obj, use_obj)\n",
    "            nodes.append((rag_score, node))\n",
    "        nodes.sort(key=lambda x: x[0])\n",
    "        filtered_nodes = [node for (rag_score,node) in nodes if rag_score > 0.5][:3]\n",
    "        if filtered_nodes:\n",
    "            prompt += \"\\nThe following are passages from the lecture notes which can (but not necessary to) help you answer the question:\\n\"\n",
    "            for node in filtered_nodes:\n",
    "                prompt += f\"{node.node.text}\\n\"\n",
    "    messages = [\n",
    "            ChatMessage(role=\"system\", content=prompt),\n",
    "            ChatMessage(role=\"user\", content=question),\n",
    "        ]\n",
    "    response = llm.chat(messages).message.content\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_response(question):\n",
    "    prompt = \"You are a teaching assistant helping a student answer different questions.\"\n",
    "    retrieved_nodes = retriever.retrieve(question) \n",
    "    prompt += \"\\nThe following are passages from the lecture notes which can (but not necessary to) help you answer the question:\\n\"\n",
    "    for node in retrieved_nodes:\n",
    "        prompt += f\"{node.node.text}\\n\"\n",
    "    messages = [\n",
    "            ChatMessage(role=\"system\", content=prompt),\n",
    "            ChatMessage(role=\"user\", content=question),\n",
    "        ]\n",
    "    response = llm.chat(messages).message.content\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(question):\n",
    "    prompt = \"You are a teaching assistant helping a student answer different questions.\"\n",
    "    messages = [\n",
    "                ChatMessage(role=\"system\", content=prompt),\n",
    "                ChatMessage(role=\"user\", content=question),\n",
    "            ]\n",
    "    response = llm.chat(messages).message.content\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('questions.json')\n",
    "questions = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = []\n",
    "for question in questions:\n",
    "    student_date = \"12/15/2022\"\n",
    "    s = selfrag_response(question[\"question\"], student_date)\n",
    "    r = rag_response(question[\"question\"])\n",
    "    g = get_response(question[\"question\"])\n",
    "    a = question[\"answer\"]\n",
    "    sa = cosine_distance(embed(s), embed(a))\n",
    "    ra = cosine_distance(embed(r), embed(a))\n",
    "    ga = cosine_distance(embed(g), embed(a))\n",
    "    responses.append({\"selfrag\": s, \"rag\": r, \"normal\": g, \"question\": question[\"question\"], \"answer\": question[\"answer\"], \"lecture\": question[\"lecture\"], \"sa\": sa, \"ra\": ra, \"ga\":ga })\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data.json\", \"w\") as f:\n",
    "     json.dump(responses, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08566703527204451 0.07534330891024682 0.09122637491072377\n"
     ]
    }
   ],
   "source": [
    "sa_avg = 0\n",
    "ra_avg = 0\n",
    "ga_avg = 0\n",
    "for result in responses:\n",
    "    sa_avg += result[\"sa\"]\n",
    "    ra_avg += result[\"ra\"]\n",
    "    ga_avg += result[\"ga\"]\n",
    "sa_avg /= len(responses)\n",
    "ra_avg /= len(responses)\n",
    "ga_avg /= len(responses)\n",
    "print(sa_avg, ra_avg, ga_avg)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c5054bc688dac1822fd64089d6a193023ad0b01e94ea25c52cda134d5f99a0ef"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
