{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex,SimpleDirectoryReader,ServiceContext\n",
    "from dotenv import load_dotenv\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.llms import ChatMessage\n",
    "from llama_index.embeddings.langchain import LangchainEmbedding\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "import os\n",
    "from llama_index.core.prompts import PromptTemplate\n",
    "from lecture_formatting import lecture_dates6100L, get_status, create_documents, lecture_mapping\n",
    "from datetime import datetime\n",
    "from pydantic import BaseModel\n",
    "from enum import Enum\n",
    "from typing_extensions import Literal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IsSeen(BaseModel):\n",
    "    \"\"\"An object determining if a document has been seen or not\"\"\"\n",
    "    seen: bool\n",
    "class IsSup(BaseModel):\n",
    "    \"\"\"An object determining if a document fully support, partially supprt or none support for an answer\"\"\"\n",
    "    sup: Literal[\"fully\", \"partial\", \"none\"]\n",
    "    \n",
    "class IsUse(BaseModel):\n",
    "    \"\"\"An object determining how useful an answer is from 1 to 5\"\"\"\n",
    "    use: Literal[\"1\", \"2\", \"3\", \"4\", \"5\"]\n",
    "\n",
    "class IsRel(BaseModel):\n",
    "    \"\"\"An object determine if the retrieved passage is relevant to the question\"\"\"\n",
    "    rel: bool\n",
    "\n",
    "class IsRet(BaseModel):\n",
    "    \"\"\"An object determining whether a question requires retrieval of external documents\"\"\"\n",
    "    ret: bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_llm(model):\n",
    "    \"\"\"\n",
    "    Function that return LLMs given model and extra arguments\n",
    "    TODO: Add more LLMs\n",
    "    \"\"\"\n",
    "    return OpenAI(model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(seen_obj, rel_obj, sup_obj, use_obj):\n",
    "    w = [0.2, 0.3, 0.2, 0.4]\n",
    "    seen = 1 if seen_obj.seen else 0\n",
    "    rel = 1 if rel_obj.rel else 0\n",
    "    sup = 0\n",
    "    if sup_obj.sup == \"fully\":\n",
    "        sup = 1\n",
    "    elif sup_obj.sup == \"partial\":\n",
    "        sup = 0.5\n",
    "    use = int(use_obj.use)\n",
    "    return w[0]*seen + w[1]*rel + w[2]*sup + w[3]*use \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gyalpoaguirre/Dropbox/Mac/Documents/mit/6.s893/self-rag-education/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/gyalpoaguirre/Dropbox/Mac/Documents/mit/6.s893/self-rag-education/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/var/folders/q_/gmxdkf893w3bm9wxvh6635t80000gp/T/ipykernel_53460/2349502471.py:4: DeprecationWarning: Call to deprecated class method from_defaults. (ServiceContext is deprecated, please use `llama_index.settings.Settings` instead.) -- Deprecated since version 0.10.0.\n",
      "  service_context=ServiceContext.from_defaults(\n"
     ]
    }
   ],
   "source": [
    "embed_model=LangchainEmbedding(HuggingFaceEmbeddings(model_name=\"mixedbread-ai/mxbai-embed-large-v1\"))\n",
    "llm = get_llm(\"gpt-4\")\n",
    "\n",
    "service_context=ServiceContext.from_defaults(\n",
    "    chunk_size=1024,\n",
    "    llm=llm,\n",
    "    embed_model=embed_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents=SimpleDirectoryReader(\"./formatted_lectures\").load_data()\n",
    "index=VectorStoreIndex.from_documents(documents,service_context=service_context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = index.as_retriever(similarity_top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(question, student_date):\n",
    "    response = \"\"\n",
    "    prompt = \"You are a teaching assistant helping a student answer different questions.\"\n",
    "    retrieval_tmpl = PromptTemplate(\"Generate a Retrieval object given this question {question}\")\n",
    "    ret_obj = llm.structured_predict(IsRet, retrieval_tmpl, question=question)\n",
    "    if ret_obj.ret:\n",
    "        retrieved_nodes = retriever.retrieve(question) \n",
    "        seen_tmpl = PromptTemplate(\"Generate an IsSeen object given the fact that the student has seen all lectures up to this date {student_date} and this current lecture has been given on this date {lecture_date}.\")\n",
    "        rel_tmpl = PromptTemplate(\"Generate an IsRel object given a passage:{passage} and question: {question} toetermine if this passage is relevant to this question\")\n",
    "        use_tmpl  = PromptTemplate(\"Generate an IsUse object by ranking how useful the response: {response} for this question: {question} is from 1 to 5\")\n",
    "        sup_tmpl = PromptTemplate(\"Generate an IsSup object by evaluating how supportive the response: {response} for this question:{question}, whether it is 'fully', 'partial' or 'none' support.\")\n",
    "        nodes = []\n",
    "        for node in retrieved_nodes:\n",
    "            lecture = node.node.metadata[\"file_name\"].split(\".md\")[0]\n",
    "            lecture_date = lecture_dates6100L[lecture]\n",
    "            seen_obj = llm.structured_predict(IsSeen, seen_tmpl, student_date=student_date, lecture_date=lecture_date)\n",
    "            passage = node.node.text\n",
    "            rel_obj = llm.structured_predict(IsRel, rel_tmpl, passage=passage, question=question)\n",
    "            temp_prompt = prompt + f\"\\nThe following is an excerpt which could be useful for answering this question: {passage}\"\n",
    "            messages = [\n",
    "                ChatMessage(role=\"system\", content=temp_prompt),\n",
    "                ChatMessage(role=\"user\", content=question),\n",
    "            ]\n",
    "            temp_response =  llm.chat(messages).message.content\n",
    "            use_obj = llm.structured_predict(IsUse, use_tmpl, response=temp_response, question=question)\n",
    "            sup_obj = llm.structured_predict(IsSup, sup_tmpl, response=temp_response, question=question)\n",
    "            rag_score = get_score(seen_obj, rel_obj, sup_obj, use_obj)\n",
    "            nodes.append((rag_score, node))\n",
    "        nodes.sort(key=lambda x: x[0])\n",
    "        filtered_nodes = [node for (rag_score,node) in nodes if rag_score > 0.5][:3]\n",
    "        if filtered_nodes:\n",
    "            prompt += \"\\nThe following are passages from the lecture notes which can (but not necessary to) help you answer the question:\\n\"\n",
    "            for node in filtered_nodes:\n",
    "                prompt += f\"{node.node.text}\\n\"\n",
    "    messages = [\n",
    "            ChatMessage(role=\"system\", content=prompt),\n",
    "            ChatMessage(role=\"user\", content=question),\n",
    "        ]\n",
    "    response = llm.chat(messages).message.content\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = get_response(\"What is a list in python?\", \"10/24/2022\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"A list in Python is a built-in data structure that can hold a collection of items. These items can be of different types including integers, strings, other lists, dictionaries, etc. Lists are mutable, meaning their elements can be changed after they are created. They are ordered, which means that the items have a defined order that will not change unless you do so explicitly. Lists are defined by having values between square brackets [ ] and items are separated by commas. For example, a list could be: [1, 'a', [2, 3], 'hello'].\""
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c5054bc688dac1822fd64089d6a193023ad0b01e94ea25c52cda134d5f99a0ef"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
